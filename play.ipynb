{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight ---------------------------------- torch.Size([50257, 768])\n",
      "transformer.wpe.weight ---------------------------------- torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.0.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.0.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.0.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.1.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.1.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.1.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.1.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.2.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.2.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.2.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.2.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.3.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.3.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.3.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.3.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.4.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.4.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.4.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.4.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.5.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.5.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.5.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.5.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.6.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.6.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.6.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.6.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.7.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.7.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.7.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.7.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.8.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.8.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.8.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.8.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.9.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.9.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.9.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.9.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.10.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.10.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.10.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.10.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.11.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.11.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.11.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.11.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.ln_f.weight ---------------------------------- torch.Size([768])\n",
      "transformer.ln_f.bias ---------------------------------- torch.Size([768])\n",
      "lm_head.weight ---------------------------------- torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "model_hf_sd = model_hf.state_dict()\n",
    "for k, v in model_hf_sd.items():\n",
    "    print(k,\"----------------------------------\" ,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradeformer.model import TradeFormer, TradeFormerConfig\n",
    "from tradeformer.tokenizer import MarketTokenizer\n",
    "\n",
    "\n",
    "with open('./Data/preprocessed_data/train.txt', 'r') as f:\n",
    "    train_data = f.readlines()\n",
    "\n",
    "\n",
    "cfg = TradeFormerConfig()\n",
    "model = TradeFormer(cfg)\n",
    "tokenizer = MarketTokenizer('./trained_tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 1000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "test_inp = torch.randint(0, 100, (5, 8), dtype=torch.long)\n",
    "model(test_inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['AJM AIK AEO BGO BIM AHN BGQ AHO BJO BJM',\n",
       "  'AJM AIK AEO BGO BIM AHN BGQ AHO BJO BJM',\n",
       "  'AJM AIK AEO BGO BIM AHN BGQ AHO BJO BJM',\n",
       "  'AJM AIK AEO BGO BIM AHN BGQ AHO BJO BJM',\n",
       "  'AJM AIK AEO BGO BIM AHN BGQ AHO BJO BJM'],\n",
       " tensor([[ 44,  35, 171,  86, 785, 166,  93],\n",
       "         [ 44,  35, 171,  86, 785, 166,  93],\n",
       "         [ 44,  35, 171,  86, 785, 166,  93],\n",
       "         [ 44,  35, 171,  86, 785, 166,  93],\n",
       "         [ 44,  35, 171,  86, 785, 166,  93]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp_str = [(' '.join(x.split()[:10])) for x in train_data[0:5]]\n",
    "test_inp_toknized = torch.tensor([tokenizer.encode(test_inp_str[i]) for i in range(len(test_inp_str))])\n",
    "test_inp_str, test_inp_toknized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0732, -0.6493, -0.5046,  ..., -0.3713,  0.5752, -0.0057],\n",
       "         [-0.1772, -0.1031, -0.2421,  ..., -0.9707, -0.2175,  0.1517],\n",
       "         [-0.5231,  0.6064,  0.1934,  ..., -0.3642, -0.0319,  0.6557],\n",
       "         ...,\n",
       "         [ 0.1375, -0.3910, -0.8236,  ..., -0.2809, -0.2844, -0.7719],\n",
       "         [-0.8966,  0.7704, -0.5022,  ..., -0.4647, -1.1333,  0.4188],\n",
       "         [ 1.1706, -0.0215,  0.0896,  ..., -0.1421, -0.0074, -0.3911]],\n",
       "\n",
       "        [[-0.0732, -0.6493, -0.5046,  ..., -0.3713,  0.5752, -0.0057],\n",
       "         [-0.1772, -0.1031, -0.2421,  ..., -0.9707, -0.2175,  0.1517],\n",
       "         [-0.5231,  0.6064,  0.1934,  ..., -0.3642, -0.0319,  0.6557],\n",
       "         ...,\n",
       "         [ 0.1375, -0.3910, -0.8236,  ..., -0.2809, -0.2844, -0.7719],\n",
       "         [-0.8966,  0.7704, -0.5022,  ..., -0.4647, -1.1333,  0.4188],\n",
       "         [ 1.1706, -0.0215,  0.0896,  ..., -0.1421, -0.0074, -0.3911]],\n",
       "\n",
       "        [[-0.0732, -0.6493, -0.5046,  ..., -0.3713,  0.5752, -0.0057],\n",
       "         [-0.1772, -0.1031, -0.2421,  ..., -0.9707, -0.2175,  0.1517],\n",
       "         [-0.5231,  0.6064,  0.1934,  ..., -0.3642, -0.0319,  0.6557],\n",
       "         ...,\n",
       "         [ 0.1375, -0.3910, -0.8236,  ..., -0.2809, -0.2844, -0.7719],\n",
       "         [-0.8966,  0.7704, -0.5022,  ..., -0.4647, -1.1333,  0.4188],\n",
       "         [ 1.1706, -0.0215,  0.0896,  ..., -0.1421, -0.0074, -0.3911]],\n",
       "\n",
       "        [[-0.0732, -0.6493, -0.5046,  ..., -0.3713,  0.5752, -0.0057],\n",
       "         [-0.1772, -0.1031, -0.2421,  ..., -0.9707, -0.2175,  0.1517],\n",
       "         [-0.5231,  0.6064,  0.1934,  ..., -0.3642, -0.0319,  0.6557],\n",
       "         ...,\n",
       "         [ 0.1375, -0.3910, -0.8236,  ..., -0.2809, -0.2844, -0.7719],\n",
       "         [-0.8966,  0.7704, -0.5022,  ..., -0.4647, -1.1333,  0.4188],\n",
       "         [ 1.1706, -0.0215,  0.0896,  ..., -0.1421, -0.0074, -0.3911]],\n",
       "\n",
       "        [[-0.0732, -0.6493, -0.5046,  ..., -0.3713,  0.5752, -0.0057],\n",
       "         [-0.1772, -0.1031, -0.2421,  ..., -0.9707, -0.2175,  0.1517],\n",
       "         [-0.5231,  0.6064,  0.1934,  ..., -0.3642, -0.0319,  0.6557],\n",
       "         ...,\n",
       "         [ 0.1375, -0.3910, -0.8236,  ..., -0.2809, -0.2844, -0.7719],\n",
       "         [-0.8966,  0.7704, -0.5022,  ..., -0.4647, -1.1333,  0.4188],\n",
       "         [ 1.1706, -0.0215,  0.0896,  ..., -0.1421, -0.0074, -0.3911]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_inp_toknized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "#TODO move the generate function into the model class\n",
    "def generate(tokens, len=5, max_len=20, num_top_samples=10, device='cpu'):\n",
    "    # TOKENS: B, T\n",
    "    tokens = tokens.to(device) \n",
    "    while tokens.size(1) < max_len:\n",
    "        with torch.no_grad():\n",
    "            out = model(tokens)\n",
    "\n",
    "            logits = out[:, -1, :]\n",
    "            probs = F.softmax(logits, 1)\n",
    "\n",
    "            topk_probs, topk_idx = torch.topk(probs, num_top_samples, dim=-1)\n",
    "\n",
    "            ix = torch.multinomial(topk_probs, 1)\n",
    "            xcol = torch.gather(topk_idx, -1, ix) #(B, 1)\n",
    "\n",
    "            tokens = torch.cat((tokens, xcol), dim=1)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "out = generate(test_inp_toknized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 7]), torch.Size([5, 20]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp_toknized.shape,  out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AJM AIK AEO BGO BIM AHN BGQ AHO BJO BJM AGO AJO BEL AHL AGL AHO AJO AGQ CEM AJO AJL BDL BEL BGM BJO AEM AHL BHN BFL AFL BFO AHL BGL BEM BEO AIO'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
