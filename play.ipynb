{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight ---------------------------------- torch.Size([50257, 768])\n",
      "transformer.wpe.weight ---------------------------------- torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.0.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.0.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.0.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.1.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.1.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.1.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.1.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.2.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.2.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.2.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.2.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.3.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.3.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.3.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.3.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.4.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.4.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.4.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.4.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.5.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.5.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.5.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.5.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.6.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.6.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.6.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.6.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.7.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.7.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.7.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.7.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.8.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.8.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.8.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.8.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.9.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.9.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.9.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.9.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.10.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.10.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.10.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.10.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.11.ln_1.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.11.ln_1.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight ---------------------------------- torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias ---------------------------------- torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight ---------------------------------- torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.11.ln_2.weight ---------------------------------- torch.Size([768])\n",
      "transformer.h.11.ln_2.bias ---------------------------------- torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight ---------------------------------- torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias ---------------------------------- torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight ---------------------------------- torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias ---------------------------------- torch.Size([768])\n",
      "transformer.ln_f.weight ---------------------------------- torch.Size([768])\n",
      "transformer.ln_f.bias ---------------------------------- torch.Size([768])\n",
      "lm_head.weight ---------------------------------- torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "model_hf_sd = model_hf.state_dict()\n",
    "for k, v in model_hf_sd.items():\n",
    "    print(k,\"----------------------------------\" ,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradeformer.model import TradeFormer, TradeFormerConfig\n",
    "from tradeformer.tokenizer import MarketTokenizer\n",
    "\n",
    "\n",
    "with open('./Data/preprocessed_data/train.txt', 'r') as f:\n",
    "    train_data = f.readlines()\n",
    "\n",
    "\n",
    "cfg = TradeFormerConfig()\n",
    "model = TradeFormer(cfg)\n",
    "tokenizer = MarketTokenizer('./trained_tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 1000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "test_inp = torch.randint(0, 100, (5, 8), dtype=torch.long)\n",
    "model(test_inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['AJM AIK AEO BGO BIM AHN BGQ AHO BJO BJM',\n",
       "  'AJM AIK AEO BGO BIM AHN BGQ AHO BJO BJM',\n",
       "  'AJM AIK AEO BGO BIM AHN BGQ AHO BJO BJM',\n",
       "  'AJM AIK AEO BGO BIM AHN BGQ AHO BJO BJM',\n",
       "  'AJM AIK AEO BGO BIM AHN BGQ AHO BJO BJM'],\n",
       " tensor([[ 44,  35, 171,  86, 785, 166,  93],\n",
       "         [ 44,  35, 171,  86, 785, 166,  93],\n",
       "         [ 44,  35, 171,  86, 785, 166,  93],\n",
       "         [ 44,  35, 171,  86, 785, 166,  93],\n",
       "         [ 44,  35, 171,  86, 785, 166,  93]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp_str = [(' '.join(x.split()[:10])) for x in train_data[0:5]]\n",
    "test_inp_toknized = torch.tensor([tokenizer.encode(test_inp_str[i]) for i in range(len(test_inp_str))])\n",
    "test_inp_str, test_inp_toknized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0732, -0.6493, -0.5046,  ..., -0.3713,  0.5752, -0.0057],\n",
       "         [-0.1772, -0.1031, -0.2421,  ..., -0.9707, -0.2175,  0.1517],\n",
       "         [-0.5231,  0.6064,  0.1934,  ..., -0.3642, -0.0319,  0.6557],\n",
       "         ...,\n",
       "         [ 0.1375, -0.3910, -0.8236,  ..., -0.2809, -0.2844, -0.7719],\n",
       "         [-0.8966,  0.7704, -0.5022,  ..., -0.4647, -1.1333,  0.4188],\n",
       "         [ 1.1706, -0.0215,  0.0896,  ..., -0.1421, -0.0074, -0.3911]],\n",
       "\n",
       "        [[-0.0732, -0.6493, -0.5046,  ..., -0.3713,  0.5752, -0.0057],\n",
       "         [-0.1772, -0.1031, -0.2421,  ..., -0.9707, -0.2175,  0.1517],\n",
       "         [-0.5231,  0.6064,  0.1934,  ..., -0.3642, -0.0319,  0.6557],\n",
       "         ...,\n",
       "         [ 0.1375, -0.3910, -0.8236,  ..., -0.2809, -0.2844, -0.7719],\n",
       "         [-0.8966,  0.7704, -0.5022,  ..., -0.4647, -1.1333,  0.4188],\n",
       "         [ 1.1706, -0.0215,  0.0896,  ..., -0.1421, -0.0074, -0.3911]],\n",
       "\n",
       "        [[-0.0732, -0.6493, -0.5046,  ..., -0.3713,  0.5752, -0.0057],\n",
       "         [-0.1772, -0.1031, -0.2421,  ..., -0.9707, -0.2175,  0.1517],\n",
       "         [-0.5231,  0.6064,  0.1934,  ..., -0.3642, -0.0319,  0.6557],\n",
       "         ...,\n",
       "         [ 0.1375, -0.3910, -0.8236,  ..., -0.2809, -0.2844, -0.7719],\n",
       "         [-0.8966,  0.7704, -0.5022,  ..., -0.4647, -1.1333,  0.4188],\n",
       "         [ 1.1706, -0.0215,  0.0896,  ..., -0.1421, -0.0074, -0.3911]],\n",
       "\n",
       "        [[-0.0732, -0.6493, -0.5046,  ..., -0.3713,  0.5752, -0.0057],\n",
       "         [-0.1772, -0.1031, -0.2421,  ..., -0.9707, -0.2175,  0.1517],\n",
       "         [-0.5231,  0.6064,  0.1934,  ..., -0.3642, -0.0319,  0.6557],\n",
       "         ...,\n",
       "         [ 0.1375, -0.3910, -0.8236,  ..., -0.2809, -0.2844, -0.7719],\n",
       "         [-0.8966,  0.7704, -0.5022,  ..., -0.4647, -1.1333,  0.4188],\n",
       "         [ 1.1706, -0.0215,  0.0896,  ..., -0.1421, -0.0074, -0.3911]],\n",
       "\n",
       "        [[-0.0732, -0.6493, -0.5046,  ..., -0.3713,  0.5752, -0.0057],\n",
       "         [-0.1772, -0.1031, -0.2421,  ..., -0.9707, -0.2175,  0.1517],\n",
       "         [-0.5231,  0.6064,  0.1934,  ..., -0.3642, -0.0319,  0.6557],\n",
       "         ...,\n",
       "         [ 0.1375, -0.3910, -0.8236,  ..., -0.2809, -0.2844, -0.7719],\n",
       "         [-0.8966,  0.7704, -0.5022,  ..., -0.4647, -1.1333,  0.4188],\n",
       "         [ 1.1706, -0.0215,  0.0896,  ..., -0.1421, -0.0074, -0.3911]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_inp_toknized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "#TODO move the generate function into the model class\n",
    "def generate(tokens, len=5, max_len=20, num_top_samples=10, device='cpu'):\n",
    "    # TOKENS: B, T\n",
    "    tokens = tokens.to(device) \n",
    "    while tokens.size(1) < max_len:\n",
    "        with torch.no_grad():\n",
    "            out = model(tokens)\n",
    "\n",
    "            logits = out[:, -1, :]\n",
    "            probs = F.softmax(logits, 1)\n",
    "\n",
    "            topk_probs, topk_idx = torch.topk(probs, num_top_samples, dim=-1)\n",
    "\n",
    "            ix = torch.multinomial(topk_probs, 1)\n",
    "            xcol = torch.gather(topk_idx, -1, ix) #(B, 1)\n",
    "\n",
    "            tokens = torch.cat((tokens, xcol), dim=1)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "out = generate(test_inp_toknized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 7]), torch.Size([5, 20]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp_toknized.shape,  out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AJM AIK AEO BGO BIM AHN BGQ AHO BJO BJM AGO AJO BEL AHL AGL AHO AJO AGQ CEM AJO AJL BDL BEL BGM BJO AEM AHL BHN BFL AFL BFO AHL BGL BEM BEO AIO'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/preprocessed_data/test.txt', 'r') as f:\n",
    "    text = f.readlines()\n",
    "\n",
    "data = ' '.join(text[0].split()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradeformer import MarketTokenizer\n",
    "\n",
    "tokenizer = MarketTokenizer('./trained_tokenizer/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80, 609, 39, 24, 79, 79, 314, 725, 32, 177, 219, 185, 469, 29, 34, 257, 538, 29, 721, 65, 76, 485, 8, 18, 76, 715, 91, 67, 72, 39, 194, 45, 562, 81, 64, 8, 85, 92, 285, 13, 18, 110, 2, 22, 9, 43, 79, 83, 30, 64, 237, 584, 59, 11, 151, 976, 11, 967, 0, 73, 12, 109, 207, 154, 74, 43, 109, 219, 178, 59, 34, 81, 47, 24, 72, 694, 62, 351, 46, 642, 88, 50, 193, 74, 27, 233, 25, 307, 79, 45, 1, 241, 43, 377, 24, 41, 31, 95, 291, 878, 10, 2, 67, 185, 36, 56, 92, 57, 58, 24, 79, 58, 8, 57, 56, 243, 106, 184, 92, 23, 362, 58, 29, 31, 34, 95, 69, 96, 46, 66, 52, 46, 81, 93, 9, 383, 201, 192, 171, 286, 20, 751, 32, 66, 16, 322, 90, 48, 184, 76, 66, 307, 195, 72, 169, 27, 448, 453, 19, 11, 22, 63, 59, 67, 468, 80, 15, 25, 29, 77, 952, 278, 80, 27, 38, 46, 110, 65, 66, 73, 8, 183, 158, 78, 39, 629, 66, 15, 60, 9, 15, 418, 92, 61, 342, 85, 296, 79, 217, 34, 185, 58, 334, 8, 23, 520, 10, 13, 92, 72, 20, 78, 16, 1, 406, 83, 70, 39, 73, 72, 80, 66, 36, 53, 22, 48, 364, 282, 93, 66, 73, 11, 15, 74, 17, 24, 22, 45, 14, 746, 32, 72, 26, 848, 79, 405, 673, 81, 97, 198, 930, 10, 46, 419, 208, 45, 60, 725, 463, 95, 64, 93, 13, 152, 149, 74, 434, 188, 769, 39, 607, 528, 31, 260, 76, 513, 459, 48, 22, 224, 37, 64, 277, 23, 461, 10, 75, 358, 388, 81, 663, 30, 72, 710, 39, 421, 523, 83, 26, 825, 9, 19, 97, 232, 60, 3, 63, 26, 44, 34, 34, 214, 95, 80, 78, 4, 10, 79, 69, 12, 186, 370, 74, 64, 46, 151, 59, 26, 44, 76, 64, 53, 24, 16, 70, 4, 67, 19, 52, 27, 160, 59, 179, 254, 498, 72, 157, 44, 18, 23, 73, 392, 4, 67, 38, 260, 585, 15, 76, 825, 59, 81, 1, 59, 20, 32, 280, 8, 387, 44, 240, 45, 281, 48, 48, 67, 685, 23, 10, 154, 254, 332, 96, 73, 72, 34, 94, 409, 78, 257, 37, 60, 96, 11, 46, 420, 60, 15, 11, 494, 35, 277, 176, 11, 73, 232, 33, 18, 17, 57, 645, 8, 15, 31, 24, 169, 161, 80, 59, 4, 12, 46, 78, 43, 20, 893, 108, 64, 92, 53, 92, 1, 361, 76, 29, 50, 57, 161, 62, 43, 67, 308, 43, 210, 31, 700, 46, 729, 59, 306, 78, 7, 24, 45, 6, 47, 95, 44, 61, 8, 235, 71, 39, 551, 4, 93, 71, 148, 18, 85, 64, 57, 45, 519, 32, 168, 423, 8, 504, 66, 60, 66, 16, 682, 11, 13, 211, 92, 58, 232, 77, 11, 48, 78, 422, 32, 24, 71, 30, 217, 679, 147, 71, 85, 22, 7, 24, 161, 32, 4, 108, 302, 8, 92, 64, 278, 13, 578, 147, 25, 57, 39, 24, 59, 201, 155, 31, 96, 44, 206, 78, 43, 467, 55, 595, 8, 39, 229, 357, 78, 4, 42, 832, 17, 261, 16, 60, 12, 22, 74, 57, 59, 611, 95, 64, 78, 172, 23, 6, 39, 577, 198, 71, 1, 424, 45, 22, 81, 191, 41, 94, 43, 83, 24, 106, 76, 45, 4, 16, 292, 65, 9, 79, 57, 19, 95, 78, 153, 663, 34, 466, 19, 24, 72, 277, 25, 10, 273, 24, 23, 847, 78, 349, 156, 8, 335, 74, 888, 166, 57, 80, 45, 209, 75, 13, 979, 258, 789, 15, 68, 20, 18, 60, 62, 93, 71, 11, 872, 23, 150, 0, 561, 70, 78, 38, 247, 8, 29, 1, 79, 541, 69, 63, 572, 97, 95, 10, 32, 59, 67, 27, 281, 32, 66, 8, 234, 24, 58, 70, 219, 93, 157, 92, 539, 39, 48, 31, 226, 20, 69, 25, 300, 18, 44, 42, 73, 72, 4, 43, 45, 5, 67, 657, 203, 345, 838, 77, 84, 15, 89, 856, 79, 37, 526, 352, 9, 18, 43, 24, 237, 62, 62, 57, 66, 34, 286, 83, 65, 43, 18, 24, 192, 10, 17, 78, 59, 167, 77, 20, 80, 25, 80, 224, 79, 17, 23, 208, 92, 14, 10, 255, 8, 557, 46, 213, 167, 18, 190, 80, 66, 29, 18, 31, 74, 85, 582, 86, 22, 262, 92, 43, 147, 71, 39, 94, 357, 46, 97, 147, 80, 81, 73, 47, 681, 53, 23, 198, 26, 9, 324, 585, 67]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(data)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 80, 609,  39,  24,  79,  79],\n",
      "        [314, 725,  32, 177, 219, 185],\n",
      "        [469,  29,  34, 257, 538,  29],\n",
      "        [721,  65,  76, 485,   8,  18]])\n",
      "tensor([[609,  39,  24,  79,  79, 314],\n",
      "        [725,  32, 177, 219, 185, 469],\n",
      "        [ 29,  34, 257, 538,  29, 721],\n",
      "        [ 65,  76, 485,   8,  18,  76]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "buf = torch.tensor(tokens[:24 + 1])\n",
    "x = buf[:-1].view(4, 6)\n",
    "y = buf[1:].view(4, 6)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:32<00:00,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 125796 tokens\n",
      "1 epoch = 982 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dataloader import DataLoaderLite\n",
    "\n",
    "tokenizer_path = './trained_tokenizer/'\n",
    "dataset_path = './Data/preprocessed_data/token.txt'\n",
    "B = 4\n",
    "T = 32\n",
    "dataloader = DataLoaderLite(tokenizer_path, dataset_path, B, T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
